{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image as PImage\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as img\n",
    "\n",
    "import importlib\n",
    "\n",
    "# from sam.sam import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findFiles(path, rec=False): return glob.glob(path,recursive=rec) \n",
    "\n",
    "def get_id (value):\n",
    "    keys = ['1', '2', '3', '3_hook', '4', '5', '6', '7', '8', 'a', 'b', 'b_nothumb', \n",
    "            'b_thumb', 'cbaby', 'obaby', 'by', 'c', 'd', 'e', 'f', 'f_open', 'fly', \n",
    "            'fly_nothumb', 'g', 'h', 'h_hook', 'h_thumb', 'i', 'jesus', 'k', 'l_hook', \n",
    "            'middle', 'm', 'n', 'o', 'index', 'index_flex', 'index_hook', 'pincet', \n",
    "            'ital', 'ital_thumb', 'ital_nothumb', 'ital_open', 'r', 's', 'write', \n",
    "            'spoon', 't', 'v', 'v_flex', 'v_hook', 'v_thumb', 'w', 'y', 'ae', \n",
    "            'ae_thumb', 'pincet_double', 'obaby_double', 'm2', 'jesus_thumb']\n",
    "    return keys.index(value)+1\n",
    "\n",
    "def getAllLabels():\n",
    "    train = pd.read_csv(r'1miohands-v2-trainingalignment.txt', sep=\" \", header=None, names=[\"path\", \"label\"])\n",
    "    train.path = train.path.apply(lambda x: x.replace('/work/cv2/koller/features/danish_nz_ph2014/hand.20151016/data/joint/','/Users/madina/Desktop/dataset/1miohands-v2-training/'))\n",
    "    \n",
    "    test = pd.read_csv(r'3359-ph2014-MS-handshape-annotations.txt', sep=\" \", header=None, names=[\"path\", \"label\"])\n",
    "    test.path = test.path.apply(lambda x: x.replace('images/','/Users/madina/Desktop/dataset/1miohands-v2-training/'))\n",
    "    test.label = test.label.apply(lambda x: get_id(x))\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def getLabel(df, path):\n",
    "    return df[df.path.str.contains(path)].label.iat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-133-17fd35f563e6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0mtestExpectedDir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/Users/madina/Desktop/dataset/1miohands-v2-training/final_phoenix_noPause_noCompound_lefthandtag_noClean'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreateLabels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0malTrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainExpectedDir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreateLabels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0malTest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtestExpectedDir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-133-17fd35f563e6>\u001B[0m in \u001B[0;36mcreateLabels\u001B[0;34m(allLabels, expectedDir)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mfileName_relative\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfindFiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexpectedDir\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\"**/*.png\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0mfileName_absolute\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbasename\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfileName_relative\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m         \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'path'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfileName_relative\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'label'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mgetLabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mallLabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfileName_relative\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'name'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfileName_absolute\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-132-2d1caf98bb52>\u001B[0m in \u001B[0;36mgetLabel\u001B[0;34m(df, path)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mgetLabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miat\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001B[0m in \u001B[0;36mcontains\u001B[0;34m(self, pat, case, flags, na, regex)\u001B[0m\n\u001B[1;32m   1560\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mregex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1561\u001B[0m         result = str_contains(self._data, pat, case=case, flags=flags, na=na,\n\u001B[0;32m-> 1562\u001B[0;31m                               regex=regex)\n\u001B[0m\u001B[1;32m   1563\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1564\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001B[0m in \u001B[0;36mstr_contains\u001B[0;34m(arr, pat, case, flags, na, regex)\u001B[0m\n\u001B[1;32m    263\u001B[0m             \u001B[0muppered\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_na_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0m_na_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muppered\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 265\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_na_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    266\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001B[0m in \u001B[0;36m_na_map\u001B[0;34m(f, arr, na_result, dtype)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_na_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna_result\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mobject\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m     \u001B[0;31m# should really _check_ for NA\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 156\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mna_result\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001B[0m in \u001B[0;36m_map\u001B[0;34m(f, arr, na_mask, na_value, dtype)\u001B[0m\n\u001B[1;32m    169\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0mconvert\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_infer_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconvert\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0;31m# Reraise the exception if callable `f` got wrong number of args.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/src/inference.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer_mask\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    254\u001B[0m                           stacklevel=3)\n\u001B[1;32m    255\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 256\u001B[0;31m         \u001B[0mf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mregex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    257\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    258\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcase\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys,glob\n",
    "import os\n",
    "\n",
    "\n",
    "def createLabels(allLabels, expectedDir):\n",
    "    df = pd.DataFrame(columns=['path', 'label', 'name'])\n",
    "\n",
    "    for fileName_relative in findFiles(expectedDir+\"**/*.png\", True):       \n",
    "        fileName_absolute = os.path.basename(fileName_relative)\n",
    "        df = df.append({'path': fileName_relative, 'label': getLabel(allLabels, fileName_relative), 'name': fileName_absolute}, ignore_index=True)\n",
    "    return df\n",
    "    \n",
    "alTrain, alTest = getAllLabels()\n",
    "trainExpectedDir = '/Users/madina/Desktop/dataset/1miohands-v2-training/danish_nz_ph2014/'\n",
    "testExpectedDir = '/Users/madina/Desktop/dataset/1miohands-v2-training/final_phoenix_noPause_noCompound_lefthandtag_noClean'\n",
    "\n",
    "train = createLabels(alTrain, trainExpectedDir)\n",
    "test = createLabels(alTest, testExpectedDir)\n",
    "\n",
    "train.to_csv(r'train.csv')\n",
    "test.to_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "def get_id (value):\n",
    "    keys = ['1', '2', '3', '3_hook', '4', '5', '6', '7', '8', 'a', 'b', 'b_nothumb', \n",
    "            'b_thumb', 'cbaby', 'obaby', 'by', 'c', 'd', 'e', 'f', 'f_open', 'fly', \n",
    "            'fly_nothumb', 'g', 'h', 'h_hook', 'h_thumb', 'i', 'jesus', 'k', 'l_hook', \n",
    "            'middle', 'm', 'n', 'o', 'index', 'index_flex', 'index_hook', 'pincet', \n",
    "            'ital', 'ital_thumb', 'ital_nothumb', 'ital_open', 'r', 's', 'write', \n",
    "            'spoon', 't', 'v', 'v_flex', 'v_hook', 'v_thumb', 'w', 'y', 'ae', \n",
    "            'ae_thumb', 'pincet_double', 'obaby_double', 'm2', 'jesus_thumb']\n",
    "    return keys.index(value)+1\n",
    "\n",
    "def createLabels(outputTrain, outputTest):\n",
    "    train = pd.read_csv(r'1miohands-v2-trainingalignment.txt', sep=\" \", header=None, names=[\"path\", \"label\"])\n",
    "    train.path = train.path.apply(lambda x: x.replace('/work/cv2/koller/features/danish_nz_ph2014/hand.20151016/data/joint/',''))\n",
    "    train.to_csv(outputTrain)\n",
    "    \n",
    "    test = pd.read_csv(r'3359-ph2014-MS-handshape-annotations.txt', sep=\" \", header=None, names=[\"path\", \"label\"])\n",
    "    test.path = test.path.apply(lambda x: x.replace('images/',''))\n",
    "    test.label = test.label.apply(lambda x: get_id(x))\n",
    "#     dups_color['label'] = df.apply (lambda row: get_id(row), axis=1)\n",
    "#     dups_color = test.pivot_table(index=['label'], aggfunc='size')\n",
    "    test.to_csv(outputTest)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = createLabels(r'train.csv', r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {1: \"1\", 2: \"2\", 3: \"3\", 4: \"3_hook\", 5: \"4\", \n",
    "          6: \"5\", 7: \"6\", 8: \"7\", 9: \"8\", 10: \"a\",\n",
    "          11: \"b\", 12: \"b_nothumb\", 13: \"b_thumb\", 14: \"cbaby\", 15: \"obaby\",\n",
    "          16: \"by\", 17: \"c\", 18: \"d\", 19: \"e\", 20: \"f\",\n",
    "          21: \"f_open\", 22: \"fly\", 23: \"fly_nothumb\", 24: \"g\", 25: \"h\",\n",
    "          26: \"h_hook\", 27: \"h_thumb\", 28: \"i\", 29: \"jesus\", 30: \"k\",\n",
    "          31: \"l_hook\", 32: \"middle\", 33: \"m\", 34: \"n\", 35: \"o\",\n",
    "          36: \"index\", 37: \"index_flex\", 38: \"index_hook\", 39: \"pincet\", 40: \"ital\",\n",
    "          41: \"ital_thumb\", 42: \"ital_nothumb\", 43: \"ital_open\", 44: \"r\", 45: \"s\",\n",
    "          46: \"write\", 47: \"spoon\", 48: \"t\", 49: \"v\", 50: \"v_flex\",\n",
    "          51: \"v_hook\", 52: \"v_thumb\", 53: \"w\", 54: \"y\", 55: \"ae\",\n",
    "          56: \"ae_thumb\", 57: \"pincet_double\", 58: \"obaby_double\", 59: \"m2\", 60: \"jesus_thumb\"}\n",
    "# mydict.values()\n",
    "keys = ['1', '2', '3', '3_hook', '4', '5', '6', '7', '8', '1', 'b', 'b_nothumb', 'b_thumb', 'cbaby', 'obaby', 'by', 'c', 'd', 'e', 'f', 'f_open', 'fly', 'fly_nothumb', 'g', 'h', 'h_hook', 'h_thumb', 'i', 'jesus', 'k', 'l_hook', 'middle', 'm', 'n', 'o', 'index', 'index_flex', 'index_hook', 'pincet', 'ital', 'ital_thumb', 'ital_nothumb', 'ital_open', 'r', 's', 'write', 'spoon', 't', 'v', 'v_flex', 'v_hook', 'v_thumb', 'w', 'y', 'ae', 'ae_thumb', 'pincet_double', 'obaby_double', 'm2', 'jesus_thumb']\n",
    "keys.index('b_nothumb')+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0\n"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "print(skimage.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.io import imread\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data , transform = None):\n",
    "        self.data = data.values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_name,label = self.data[index]\n",
    "        img_path = os.path.join(img_name)\n",
    "        image = PImage.open(img_path)\n",
    "\n",
    "#         image = imread(img_name)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDatabase():\n",
    "    \n",
    "    train_labels = pd.read_csv(r'train.csv', index_col=[0])\n",
    "    test_labels = pd.read_csv(r'test.csv', index_col=[0])\n",
    "\n",
    "    train_data, validation_data = train_test_split(train_labels, stratify=train_labels.label, test_size=0.1)\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    train_data = Dataset(train_data, train_transform )\n",
    "    validation_data = Dataset(validation_data, valid_transform )\n",
    "    test_data = Dataset(test_labels, test_transform )\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=64, num_workers=0, shuffle=True)\n",
    "    validation_loader = DataLoader(train_data, batch_size=64, num_workers=0, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=64, num_workers=0, shuffle=True)\n",
    "    \n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madina/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv(r'train.csv', index_col=[0])\n",
    "test_labels = pd.read_csv(r'test.csv', index_col=[0])\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   path  label\n",
      "0     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "1     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "2     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "3     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "4     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "5     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "6     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "7     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "8     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "9     danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "10    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "11    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "12    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "13    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "14    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "15    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "16    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "17    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "18    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "19    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "20    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "21    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "22    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "23    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "24    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "25    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "26    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "27    danish_nz_ph2014/01April_2010_Thursday_heute_d...      0\n",
      "28    danish_nz_ph2014/01April_2010_Thursday_heute_d...     13\n",
      "29    danish_nz_ph2014/01April_2010_Thursday_heute_d...     13\n",
      "...                                                 ...    ...\n",
      "3329  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3330  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3331  final_phoenix_noPause_noCompound_lefthandtag_n...      5\n",
      "3332  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3333  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3334  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3335  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3336  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3337  final_phoenix_noPause_noCompound_lefthandtag_n...     55\n",
      "3338  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3339  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3340  final_phoenix_noPause_noCompound_lefthandtag_n...     29\n",
      "3341  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3342  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3343  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3344  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3345  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3346  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3347  final_phoenix_noPause_noCompound_lefthandtag_n...     55\n",
      "3348  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3349  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3350  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3351  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3352  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3353  final_phoenix_noPause_noCompound_lefthandtag_n...      6\n",
      "3354  final_phoenix_noPause_noCompound_lefthandtag_n...     28\n",
      "3355  final_phoenix_noPause_noCompound_lefthandtag_n...     28\n",
      "3356  final_phoenix_noPause_noCompound_lefthandtag_n...     28\n",
      "3357  final_phoenix_noPause_noCompound_lefthandtag_n...     28\n",
      "3358  final_phoenix_noPause_noCompound_lefthandtag_n...     28\n",
      "\n",
      "[1236678 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, validation_data = train_test_split(train_labels, test_size=0.1)\n",
    "\n",
    "\n",
    "print(pd.concat([train_labels, test_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, padding):\n",
    "        super(ResNetBasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, 1, padding)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch   \n",
    "        self.proj = nn.Conv2d(in_ch, out_ch, 1, 2)\n",
    "        self.bn_proj = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        if (in_ch == out_ch):\n",
    "            self.block1 = ResNetBasicBlock(in_ch, out_ch, 3, 1, 1)\n",
    "        else:\n",
    "            self.block1 = ResNetBasicBlock(in_ch, out_ch, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.block2 = ResNetBasicBlock(out_ch, out_ch, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.block3 = ResNetBasicBlock(out_ch, out_ch, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if (self.in_ch == self.out_ch):\n",
    "            shortcut1 = x.clone()\n",
    "        else:\n",
    "            shortcut1 = self.proj(x)\n",
    "        x = self.block1(x)\n",
    "        x += shortcut1\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        shortcut2 = x.clone()\n",
    "        x = self.block2(x)\n",
    "        x += shortcut2\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        shortcut3 = x.clone()\n",
    "        x = self.block3(x)\n",
    "        x += shortcut3\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn0 = nn.BatchNorm2d(16)\n",
    "        self.resBlock1 = ResNetBlock(16, 16)\n",
    "        self.resBlock2 = ResNetBlock(16, 32)\n",
    "        self.resBlock3 = ResNetBlock(32, 64)\n",
    "        self.avgPool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn0(self.conv1(x)))\n",
    "\n",
    "        x = F.relu(self.resBlock1(x))\n",
    "        x = F.relu(self.resBlock2(x))\n",
    "        x = F.relu(self.resBlock3(x))\n",
    "\n",
    "        x = self.avgPool(x)\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, cross_entropy, epoch, isSam):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss = cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "#         if isSam:\n",
    "#             optimizer.first_step(zero_grad=True)\n",
    "#             cross_entropy(model(data), target).mean().backward()\n",
    "#             optimizer.second_step(zero_grad=True)\n",
    "#         else:\n",
    "#             optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "       \n",
    "def validate(model, device, validation_loader, cross_entropy):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "#             valid_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            valid_loss += cross_entropy(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    valid_loss /= len(validation_loader.dataset)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        valid_loss, correct, len(validation_loader.dataset),\n",
    "        100. * correct / len(validation_loader.dataset)))\n",
    "    return valid_loss\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            correct, total, 100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_loader, validation_loader, test_loader = loadDatabase(isData1=True)\n",
    "\n",
    "model = ResNet().to(device)\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0001)\n",
    "# sam_optimizer = SAM(model.parameters(), torch.optim.SGD, lr=0.001, momentum=0.9)\n",
    "# adadelta_optimizer = torch.optim.Adadelta(model.parameters(), lr=0.001)\n",
    "# sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "counter, train_losses, valid_losses = [], [], []\n",
    "\n",
    "for epoch in range(1, 50 + 1):\n",
    "\n",
    "    train_losses.append( train(model, device, train_loader, adam_optimizer, cross_entropy, epoch, False) )\n",
    "    valid_losses.append( validate(model, device, validation_loader, cross_entropy) )\n",
    "    counter.append(epoch)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.plot(counter, train_losses, \"r\", label = \"Train loss\")\n",
    "plt.plot(counter, valid_losses, \"b\", label = \"Validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), 'model1.ckpt') \n",
    "\n",
    "test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}